df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
df
paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i)
i = 22
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
length(df)
length(df$times)
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
start_date = as.Date("2016-09-03")
i = 22
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) != 25){
for(j in 1:length(df$sub_news_url)){
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
keyword = c(keyword,grep_keyword(sub_news_url[j]))
break
}
}
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
start_date = as.Date("2016-09-03")
i = 1
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) != 25){
for(j in 1:length(df$sub_news_url)){
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
keyword = c(keyword,grep_keyword(sub_news_url[j]))
break
}
}
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
i = 2
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
df
length(df$sub_news_url) != 25
length(df$sub_news_url) == 25
length(df$sub_news_url) == 25
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
start_date = as.Date("2016-09-03")
i = 1
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) == 25){
for(j in 1:length(df$sub_news_url)){
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
keyword = c(keyword,grep_keyword(sub_news_url[j]))
break
}
}
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
length(df$sub_news_url) == 25
df$sub_news_url
date()
start_date = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
source('~/Desktop/Dirc_Crawler/FreeNews.R', echo=TRUE)
start_date = as.character.Date(date()) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
start_date
date()
start_date = date() %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
start_date
as.Date(date())
date()
as.POSIXct(., format="%Y-%m-%d")
as.POSIXct(date(), format="%Y-%m-%d")
as.POSIXct(date(), format="%Y-%M-%d")
Sys.Date()
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
start_date = Sys.Date()
i = 1
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) != 0){
for(j in 1:length(df$sub_news_url)){
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
keyword = c(keyword,grep_keyword(sub_news_url[j]))
break
}
}
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
df
df$sub_news_url
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
start_date = Sys.Date() - 1
i = 1
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) != 0){
for(j in 1:length(df$sub_news_url)){
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
keyword = c(keyword,grep_keyword(sub_news_url[j]))
break
}
}
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
df
sub_news_url
keyword = table(keyword[-1])
keyword
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
#製作JSON格式
id_func = function(ID,No){
blogID = "%22blogId%22:"
logNo = "%22logNo%22:"
char_22 = "%22"
str_json = ""
for(i in 1:length(ID)){
blog = paste0(blogID,char_22,ID[i],char_22)
log = paste0(logNo,char_22,No[i],char_22)
str_json = paste0(str_json,"{",blog,",",log,"},")
}
return(substr(str_json, 1, nchar(str_json)-1))
}
#爬蟲
# i = 1:第幾頁
for(i in 1:20){
#先擷取LogID以及blogID
#日期可空白
start_date = "2016-07-30"
end_date = "2016-08-30"
res = paste0("http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword=%EA%B5%B0%EA%B2%83%EC%A7%88&term=&option.startDate=",
start_date,
"&option.endDate=",
end_date,
"&option.page.currentPage=",
i,
"&option.orderBy=sim")
ID = read_html(res) %>% html_nodes(".vBlogId") %>% html_attr("value")
No = read_html(res) %>% html_nodes(".vLogNo") %>% html_attr("value")
Dates = read_html(res) %>% html_nodes(".date") %>% html_text()
#再向伺服器請求LogID及blogID的資料
get_URL_JSON = paste0("http://section.blog.naver.com/TagSearchAsync.nhn?variables=[",id_func(ID,No),"]")
korea_blog = fromJSON(get_URL_JSON)
korea_blog$logNo = paste0("http://blog.naver.com/mingoesthere/",korea_blog$logNo)
#處理時間資料後再倒回
korea_blog$time = Dates %>% as.character.Date() %>% as.POSIXct(.,format="%Y.%m.%d. %H:%M")
if(i==1){
korea_blogs = korea_blog
}else{
korea_blogs = rbind(korea_blogs,korea_blog)
}
}
#Tag詞頻計算
library(jiebaR)
source('https://raw.githubusercontent.com/a5347354/BigData_Analysis/master/Demo20160731/CNCorpus.R')
# s_corpus <- CNCorpus()
# control_list=list(wordLengths=c(2,Inf),tokenize=space_tokenizer)
# s_dtm <- DocumentTermMatrix(,control=control_list)
for(i in 1:length(korea_blogs$tags)){
if(i == 1){
tags = unlist(korea_blogs$tags[i])
}else{
tags = c(tags,unlist(korea_blogs$tags[i]))
}
}
#畫圖呈現
library(plotly)
tags_tb = sort(table(tags),decreasing = FALSE)
tags_tb = tags_tb[as.numeric(tags_tb)>1]
plot_ly( x = as.numeric(tags_tb),
y = names(tags_tb),
mode = "markers")
#製作JSON格式
id_func = function(ID,No){
blogID = "%22blogId%22:"
logNo = "%22logNo%22:"
char_22 = "%22"
str_json = ""
for(i in 1:length(ID)){
blog = paste0(blogID,char_22,ID[i],char_22)
log = paste0(logNo,char_22,No[i],char_22)
str_json = paste0(str_json,"{",blog,",",log,"},")
}
return(substr(str_json, 1, nchar(str_json)-1))
}
#爬蟲
# i = 1:第幾頁
for(i in 1:20){
#先擷取LogID以及blogID
#日期可空白
start_date = "2016-07-30"
end_date = "2016-08-30"
res = paste0("http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword=%EA%B5%B0%EA%B2%83%EC%A7%88&term=&option.startDate=",
start_date,
"&option.endDate=",
end_date,
"&option.page.currentPage=",
i,
"&option.orderBy=sim")
ID = read_html(res) %>% html_nodes(".vBlogId") %>% html_attr("value")
No = read_html(res) %>% html_nodes(".vLogNo") %>% html_attr("value")
Dates = read_html(res) %>% html_nodes(".date") %>% html_text()
#再向伺服器請求LogID及blogID的資料
get_URL_JSON = paste0("http://section.blog.naver.com/TagSearchAsync.nhn?variables=[",id_func(ID,No),"]")
korea_blog = fromJSON(get_URL_JSON)
korea_blog$logNo = paste0("http://blog.naver.com/mingoesthere/",korea_blog$logNo)
#處理時間資料後再倒回
korea_blog$time = Dates %>% as.character.Date() %>% as.POSIXct(.,format="%Y.%m.%d. %H:%M")
if(i==1){
korea_blogs = korea_blog
}else{
korea_blogs = rbind(korea_blogs,korea_blog)
}
}
#Tag詞頻計算
library(jiebaR)
source('https://raw.githubusercontent.com/a5347354/BigData_Analysis/master/Demo20160731/CNCorpus.R')
# s_corpus <- CNCorpus()
# control_list=list(wordLengths=c(2,Inf),tokenize=space_tokenizer)
# s_dtm <- DocumentTermMatrix(,control=control_list)
for(i in 1:length(korea_blogs$tags)){
if(i == 1){
tags = unlist(korea_blogs$tags[i])
}else{
tags = c(tags,unlist(korea_blogs$tags[i]))
}
}
#畫圖呈現
library(plotly)
tags_tb = sort(table(tags),decreasing = FALSE)
tags_tb = tags_tb[as.numeric(tags_tb)>1]
plot_ly( x = as.numeric(tags_tb),
y = names(tags_tb),
mode = "markers")
tags
tags_tb
download.file("http://blogthumb2.naver.net/20160903_90/choamom13_1472835572405EA8sE_JPEG/0.jpg?type=s88", ..., mode = 'wb')
download.file("http://blogthumb2.naver.net/20160903_90/choamom13_1472835572405EA8sE_JPEG/0.jpg?type=s88", mode = 'wb')
download.file("http://blogthumb2.naver.net/20160903_90/choamom13_1472835572405EA8sE_JPEG/0.jpg?type=s88","123.png", mode = 'wb')
library("png", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
img <- readPNG(system.file("img", "123.png", package="png"))
img <- "http://blogthumb2.naver.net/20160903_90/choamom13_1472835572405EA8sE_JPEG/0.jpg?type=s88","123.png"
img <- "http://blogthumb2.naver.net/20160903_90/choamom13_1472835572405EA8sE_JPEG/0.jpg?type=s88"
my_image <-  readPNG(getURLContent(img))
library("RCurl", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
my_image <-  readPNG(getURLContent(img))
my_image <-  readJPEG(getURLContent(img))
install.packages("jpeg")
library("jpeg", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
my_image <-  readJPEG(getURLContent(img))
my_imgae
my_image
plot(my_image)
library('ReadImages')
detach("package:jpeg", unload=TRUE)
install.packages("readImages")
library(jpeg)
library(biOps)
detach("package:jpeg", unload=TRUE)
library("jpeg", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
img <- "http://postfiles1.naver.net/20160810_64/lovekyunjae_1470830684809Vi3VO_JPEG/DSC06761.JPG?type=w966"
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
rasterImage(my_image)
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,400,400,1,1)
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
img <- "http://postfiles1.naver.net/20160810_64/lovekyunjae_1470830684809Vi3VO_JPEG/DSC06761.JPG?type=w966"
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
library(jpeg)
img <- "http://postfiles1.naver.net/20160810_64/lovekyunjae_1470830684809Vi3VO_JPEG/DSC06761.JPG?type=w966"
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
#顯示圖片
library(jpeg)
img <- "http://postfiles1.naver.net/20160810_64/lovekyunjae_1470830684809Vi3VO_JPEG/DSC06761.JPG?type=w966"
my_image <-  readJPEG(getURLContent(img))
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(my_image,0,0,1,1)
#製作JSON格式
id_func = function(ID,No){
blogID = "%22blogId%22:"
logNo = "%22logNo%22:"
char_22 = "%22"
str_json = ""
for(i in 1:length(ID)){
blog = paste0(blogID,char_22,ID[i],char_22)
log = paste0(logNo,char_22,No[i],char_22)
str_json = paste0(str_json,"{",blog,",",log,"},")
}
return(substr(str_json, 1, nchar(str_json)-1))
}
#爬蟲
# i = 1:第幾頁
for(i in 1:20){
#先擷取LogID以及blogID
#日期可空白
start_date = "2016-07-30"
end_date = "2016-08-30"
res = paste0("http://section.blog.naver.com/sub/SearchBlog.nhn?type=post&option.keyword=%EA%B5%B0%EA%B2%83%EC%A7%88&term=&option.startDate=",
start_date,
"&option.endDate=",
end_date,
"&option.page.currentPage=",
i,
"&option.orderBy=sim")
ID = read_html(res) %>% html_nodes(".vBlogId") %>% html_attr("value")
No = read_html(res) %>% html_nodes(".vLogNo") %>% html_attr("value")
Dates = read_html(res) %>% html_nodes(".date") %>% html_text()
#再向伺服器請求LogID及blogID的資料
get_URL_JSON = paste0("http://section.blog.naver.com/TagSearchAsync.nhn?variables=[",id_func(ID,No),"]")
korea_blog = fromJSON(get_URL_JSON)
korea_blog$logNo = paste0("http://blog.naver.com/mingoesthere/",korea_blog$logNo)
#處理時間資料後再倒回
korea_blog$time = Dates %>% as.character.Date() %>% as.POSIXct(.,format="%Y.%m.%d. %H:%M")
if(i==1){
korea_blogs = korea_blog
}else{
korea_blogs = rbind(korea_blogs,korea_blog)
}
}
#Tag詞頻計算
library(jiebaR)
source('https://raw.githubusercontent.com/a5347354/BigData_Analysis/master/Demo20160731/CNCorpus.R')
# s_corpus <- CNCorpus()
# control_list=list(wordLengths=c(2,Inf),tokenize=space_tokenizer)
# s_dtm <- DocumentTermMatrix(,control=control_list)
for(i in 1:length(korea_blogs$tags)){
if(i == 1){
tags = unlist(korea_blogs$tags[i])
}else{
tags = c(tags,unlist(korea_blogs$tags[i]))
}
}
#畫圖呈現
library(plotly)
tags_tb = sort(table(tags),decreasing = FALSE)
tags_tb = tags_tb[as.numeric(tags_tb)>1]
plot_ly( x = as.numeric(tags_tb),
y = names(tags_tb),
mode = "markers")
library(rvest)
# 擷取該篇關鍵字
grep_keyword = function(url){
free_url = read_html(url)
free_keyword = free_url %>% html_nodes(".con_keyword") %>% html_nodes("a") %>% html_text()
return(free_keyword)
}
# 抓取某日期後所有頁面的關鍵字
# 今日日期
start_date = Sys.Date()
i = 1
keyword = ""
repeat{
news_url = read_html(paste0("http://news.ltn.com.tw/list/BreakingNews?page=",i))
# 分析html萃取時間
times = news_url %>% html_nodes(".lipic span") %>% html_text()
# 轉換成日期格式
times = as.character.Date(times) %>% as.POSIXct(., format="%Y-%m-%d %H:%M")
# 分析html萃取子url
sub_news_url = news_url %>% html_nodes(".picword") %>% html_attr("href")
df = data.frame(times = times,sub_news_url = sub_news_url)
df = subset(df,as.Date(df$times) >= start_date)
print(paste0("第",i,"頁完成"))
if(length(df$sub_news_url) != 0){
for(j in 1:length(df$sub_news_url)){
# 抓取子頁keyword
keyword = c(keyword,grep_keyword(sub_news_url[j]))
}
i = i+1
}else{
break
}
}
#不重複，並把第一個空字元刪除
keyword = unique(keyword[-1])
#寫入txt裡
fileConn<-file("direc.txt")
writeLines(keyword, fileConn)
close(fileConn)
keyword
free_keyword
news_url
times
sub_news_url
df
runApp()
